{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-step GRPO (vLLM) for Display Semiconductor Q&A\n",
        "\n",
        "This notebook runs a multi-step GRPO-style loop:\n",
        "- Step-wise rollouts with experience injection (per-problem)\n",
        "- Grading per attempt → experience generation per problem\n",
        "- Artifacts saved under `workspace/semiconductor_grpo_vllm/step_{k}`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "import glob\n",
        "import hashlib\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Set\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from training_free_grpo.semiconductor.prompts import (\n",
        "    PROBLEM_WITH_EXPERIENCE_TEMPLATE,\n",
        "    SINGLE_ROLLOUT_GRADING_TEMPLATE,\n",
        "    SINGLE_QUERY_CRITIQUE_TEMPLATE,\n",
        ")\n",
        "\n",
        "from training_free_grpo.semiconductor.utils import (\n",
        "    load_local_dataset,\n",
        "    load_experiences_for_prompt,\n",
        "    save_experiences_for_problem,\n",
        "    safe_json_obj,\n",
        "    safe_json_array,\n",
        "    compute_problem_key,\n",
        "    extract_final_answer,\n",
        "    to_qwen_thinking_chat,\n",
        "    run_search,\n",
        "    format_experiences_for_prompt,\n",
        "    format_requirements_block,\n",
        "    build_or_load_index,\n",
        ")\n",
        "from training_free_grpo.semiconductor.embeddings import build_model as build_embed_engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Config ----\n",
        "MODEL_PATH = os.environ.get(\"VLLM_MODEL\", \"/mnt/storage/models/Qwen3/Qwen3-32B\")\n",
        "GRADING_MODEL_PATH = os.environ.get(\"VLLM_MODEL_GRADING\", \"/mnt/storage/models/Qwen3/Qwen3-Next-80B-A3B-Thinking\")\n",
        "DATASET_PATH = os.environ.get(\"SEMI_DATASET\", \"/mnt/workspace/MLLM/zz/training_free_grpo/semiconductor/data/3.5k_filtered_processed_data.json\")\n",
        "EXPERIMENT_DIR = os.environ.get(\"SEMI_EXP_DIR\", \"/mnt/workspace/MLLM/zz/training_free_grpo/semiconductor\")\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "GRPO_N = 5\n",
        "TEMPERATURE = 0.7\n",
        "MAX_NEW_TOKENS = 16384\n",
        "NUM_STEPS = 3\n",
        "\n",
        "# Randomization\n",
        "RANDOM_SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
        "SHUFFLE_EACH_STEP = os.environ.get(\"SHUFFLE_EACH_STEP\", \"True\").lower() in [\"1\", \"true\", \"yes\"]\n",
        "\n",
        "# Grading/critique generation params\n",
        "GRADING_TEMPERATURE = 0.1\n",
        "GRADING_MAX_NEW_TOKENS = 16384\n",
        "\n",
        "# GPU/engine configurations\n",
        "TENSOR_PARALLEL_SIZE = int(os.environ.get(\"TP_SIZE\", \"8\"))\n",
        "MAX_MODEL_LEN = int(os.environ.get(\"MAX_MODEL_LEN\", \"32768\"))\n",
        "DTYPE = os.environ.get(\"DTYPE\", \"auto\")  # \"auto\", \"float16\", \"bfloat16\"\n",
        "\n",
        "# Optional separate grading engine configs (fallback to main if unset)\n",
        "GRADING_TP_SIZE = int(os.environ.get(\"GRADING_TP_SIZE\", str(TENSOR_PARALLEL_SIZE)))\n",
        "GRADING_MAX_MODEL_LEN = int(os.environ.get(\"GRADING_MAX_MODEL_LEN\", str(MAX_MODEL_LEN)))\n",
        "GRADING_DTYPE = os.environ.get(\"GRADING_DTYPE\", DTYPE)\n",
        "\n",
        "# Per-problem experiences base directory (single path)\n",
        "# Pass via env EXPERIENCES_ROOT to set the base directory. We'll create two subfolders: old/ and new/\n",
        "EXPERIENCES_BASE = os.environ.get(\"EXPERIENCES_ROOT\", os.path.join(EXPERIMENT_DIR, \"experiences\"))\n",
        "EXPERIENCES_OLD_ROOT = os.path.join(EXPERIENCES_BASE, \"old\")\n",
        "EXPERIENCES_NEW_ROOT = os.path.join(EXPERIENCES_BASE, \"new\")\n",
        "\n",
        "# Retrieval config\n",
        "TOP_K_EXPERIENCES = int(os.environ.get(\"TOP_K_EXPERIENCES\", \"5\"))\n",
        "EMBED_MODEL_ID = os.environ.get(\"EMBED_MODEL_ID\", \"Qwen/Qwen3-Embedding-8B\")\n",
        "EMBED_DEVICE = os.environ.get(\"EMBED_DEVICE\")  # e.g., \"cuda\" or None\n",
        "EMBED_BATCH_SIZE = int(os.environ.get(\"EMBED_BATCH_SIZE\", \"256\"))\n",
        "EMBED_USE_FA2 = os.environ.get(\"EMBED_USE_FA2\", \"false\").lower() in [\"1\", \"true\", \"yes\"]\n",
        "EMBED_INSTRUCTION = os.environ.get(\"EMBED_INSTRUCTION\", \"\").strip() or None\n",
        "EXPERIENCES_INDEX_DIR = os.environ.get(\"EXPERIENCES_INDEX_DIR\", os.path.join(EXPERIENCES_BASE, \"index\"))\n",
        "EMBED_REBUILD_INDEX = os.environ.get(\"EMBED_REBUILD_INDEX\", \"false\").lower() in [\"1\", \"true\", \"yes\"]\n",
        "\n",
        "# Scoring outputs\n",
        "SCORES_DIR = os.path.join(EXPERIMENT_DIR, \"scores\")\n",
        "\n",
        "# Apply CUDA visibility\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3, 4, 5, 6, 7'\n",
        "\n",
        "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
        "os.makedirs(EXPERIENCES_OLD_ROOT, exist_ok=True)\n",
        "os.makedirs(EXPERIENCES_NEW_ROOT, exist_ok=True)\n",
        "os.makedirs(SCORES_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Load dataset ----\n",
        "data: List[Dict] = load_local_dataset(DATASET_PATH)\n",
        "print(f\"Loaded {len(data)} records\")\n",
        "\n",
        "# Build index list and (optionally) shuffle once here\n",
        "random.seed(RANDOM_SEED)\n",
        "data_indices = list(range(len(data)))\n",
        "if not SHUFFLE_EACH_STEP:\n",
        "    random.shuffle(data_indices)\n",
        "    print(\"Shuffled once at start.\")\n",
        "else:\n",
        "    print(\"Will reshuffle indices each step.\")\n",
        "\n",
        "num_batches = (len(data) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "print(f\"Batching with BATCH_SIZE={BATCH_SIZE} → {num_batches} batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Build generators ----\n",
        "print(\"Building vLLM engines ...\")\n",
        "if MODEL_PATH == GRADING_MODEL_PATH:\n",
        "    gen = LLM(\n",
        "        model=GRADING_MODEL_PATH,\n",
        "        trust_remote_code=True,\n",
        "        tensor_parallel_size=GRADING_TP_SIZE,\n",
        "        max_model_len=GRADING_MAX_MODEL_LEN,\n",
        "        dtype=GRADING_DTYPE,\n",
        "        gpu_memory_utilization=0.7,\n",
        "    )\n",
        "    grade_gen = gen\n",
        "else:\n",
        "    gen = LLM(\n",
        "        model=MODEL_PATH,\n",
        "        trust_remote_code=True,\n",
        "        tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
        "        max_model_len=MAX_MODEL_LEN,\n",
        "        dtype=DTYPE,\n",
        "        gpu_memory_utilization=0.2,\n",
        "    )\n",
        "    grade_gen = LLM(\n",
        "        model=GRADING_MODEL_PATH,\n",
        "        trust_remote_code=True,\n",
        "        tensor_parallel_size=GRADING_TP_SIZE,\n",
        "        max_model_len=GRADING_MAX_MODEL_LEN,\n",
        "        dtype=GRADING_DTYPE,\n",
        "        gpu_memory_utilization=0.7,\n",
        "    )\n",
        "\n",
        "# Preload tokenizers to pass directly\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "grade_tokenizer = AutoTokenizer.from_pretrained(GRADING_MODEL_PATH)\n",
        "\n",
        "# Build embedding engine (once, before main loop)\n",
        "print(\"Building embedding engine ...\")\n",
        "embed_engine = build_embed_engine(EMBED_MODEL_ID)\n",
        "print(f\"Embedding engine ready: {EMBED_MODEL_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Multi-step rollout & experience update ----\n",
        "for step in range(NUM_STEPS):\n",
        "    # Precompute embeddings for NEW experiences at the beginning of this step\n",
        "    _records, _emb = build_or_load_index(\n",
        "        experiences_dir=EXPERIENCES_OLD_ROOT,\n",
        "        index_dir=EXPERIENCES_INDEX_DIR,\n",
        "        model=embed_engine,\n",
        "        batch_size=EMBED_BATCH_SIZE,\n",
        "        instruction=EMBED_INSTRUCTION\n",
        "    )\n",
        "    print(f\"[Step {step}] New experiences embeddings built: {_emb.shape if _emb is not None else 'n/a'}\")\n",
        "\n",
        "    # Prepare per-step state\n",
        "    step_total = 0.0\n",
        "    ops_dir = os.path.join(EXPERIMENT_DIR, \"ops\", f\"step_{step}\")\n",
        "    # Recreate ops dir for this step\n",
        "    shutil.rmtree(ops_dir, ignore_errors=True)\n",
        "    os.makedirs(ops_dir, exist_ok=True)\n",
        "\n",
        "    # Scoring state\n",
        "    cur_problem_totals: Dict[str, float] = {}\n",
        "\n",
        "    # Prepare per-step randomized indices\n",
        "    if SHUFFLE_EACH_STEP:\n",
        "        random.seed(RANDOM_SEED + step)\n",
        "        step_indices = list(data_indices)\n",
        "        random.shuffle(step_indices)\n",
        "    else:\n",
        "        step_indices = data_indices\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start = batch_idx * BATCH_SIZE\n",
        "        end = min(len(data), (batch_idx + 1) * BATCH_SIZE)\n",
        "        idx_slice = step_indices[start:end]\n",
        "        batch_data = [data[i] for i in idx_slice]\n",
        "\n",
        "        # 1) Build prompts with top-k retrieved experiences (exclude same-problem experiences)\n",
        "        formatted_batch: List[Dict] = []\n",
        "        problems = [s['problem'] for s in batch_data]\n",
        "        keys = [compute_problem_key(p) for p in problems]\n",
        "        # Over-fetch then filter to ensure we still have K after exclusion\n",
        "        retrieve_k = max(TOP_K_EXPERIENCES * 3, TOP_K_EXPERIENCES)\n",
        "        retrieved_lists = run_search(\n",
        "            experiences_dir=EXPERIENCES_OLD_ROOT,\n",
        "            query=problems,\n",
        "            top_k=retrieve_k,\n",
        "            device=None,\n",
        "            batch_size=EMBED_BATCH_SIZE,\n",
        "            use_flash_attention_2=False,\n",
        "            instruction=EMBED_INSTRUCTION,\n",
        "            index_dir=EXPERIENCES_INDEX_DIR,\n",
        "            rebuild_index=False,\n",
        "            engine=embed_engine,\n",
        "        )\n",
        "        for sample, problem_text, current_key, retrieved in zip(batch_data, problems, keys, retrieved_lists or []):\n",
        "            # Exclude experiences belonging to this problem\n",
        "            filtered = [it for it in (retrieved or []) if it.get('problem_key') != current_key]\n",
        "            top_items = filtered[:TOP_K_EXPERIENCES]\n",
        "            formatted_experiences = format_experiences_for_prompt(top_items)\n",
        "            prompt = PROBLEM_WITH_EXPERIENCE_TEMPLATE.format(\n",
        "                experiences=formatted_experiences,\n",
        "                problem=problem_text,\n",
        "            )\n",
        "            formatted_batch.append({\n",
        "                \"prompt\": prompt,\n",
        "                **sample,\n",
        "            })\n",
        "\n",
        "        # 2) GRPO duplicate\n",
        "        formatted_batch = formatted_batch * GRPO_N\n",
        "        prompts = [x[\"prompt\"] for x in formatted_batch]\n",
        "        print(f\"[Step {step}][Batch {batch_idx+1}/{num_batches}] Generating for {len(prompts)} prompts ...\")\n",
        "        \n",
        "\n",
        "        # 3) Generate\n",
        "        chat_prompts = [to_qwen_thinking_chat(p, gen_tokenizer) for p in prompts]\n",
        "        sampling_params = SamplingParams(\n",
        "            temperature=TEMPERATURE,\n",
        "            max_tokens=MAX_NEW_TOKENS,\n",
        "            top_p=0.95,\n",
        "            top_k=20,\n",
        "        )\n",
        "        outputs = gen.generate(chat_prompts, sampling_params)\n",
        "        gen_texts = [extract_final_answer(o.outputs[0].text) for o in outputs]\n",
        "\n",
        "        # 4) Attach outputs (defer rewards until grading)\n",
        "        rollouts: List[Dict] = []\n",
        "        for item, out in zip(formatted_batch, gen_texts):\n",
        "            r = dict(item)\n",
        "            r[\"response\"] = out\n",
        "            r[\"reward\"] = 0.0\n",
        "            rollouts.append(r)\n",
        "\n",
        "        # 5) Group by problem\n",
        "        problem_to_rollouts: Dict[str, List[Dict]] = {}\n",
        "        for r in rollouts:\n",
        "            problem_to_rollouts.setdefault(r[\"problem\"], []).append(r)\n",
        "\n",
        "        # 6) Batched grading for all attempts in this batch\n",
        "        grading_prompts: List[str] = []\n",
        "        grading_refs: List[tuple[str, int, Dict]] = []  # (problem, attempt_idx, rollout_ref)\n",
        "        for problem, rs in problem_to_rollouts.items():\n",
        "            req_block = rs[0].get(\"keypoints\")\n",
        "            req_text = format_requirements_block(req_block) if isinstance(req_block, list) else \"\"\n",
        "            for i, each in enumerate(rs):\n",
        "                grading_prompts.append(\n",
        "                    SINGLE_ROLLOUT_GRADING_TEMPLATE.format(\n",
        "                        problem=problem,\n",
        "                        response=str(each.get(\"response\", \"\")),\n",
        "                        requirements=req_text,\n",
        "                    )\n",
        "                )\n",
        "                grading_refs.append((problem, i, each))\n",
        "\n",
        "        grading_chat_prompts = [to_qwen_thinking_chat(p, grade_tokenizer) for p in grading_prompts]\n",
        "        g_params = SamplingParams(\n",
        "            temperature=GRADING_TEMPERATURE,\n",
        "            max_tokens=GRADING_MAX_NEW_TOKENS,\n",
        "            top_p=0.95,\n",
        "            top_k=20,\n",
        "        )\n",
        "        g_outs = grade_gen.generate(grading_chat_prompts, g_params)\n",
        "\n",
        "        problem_to_grading_pieces: Dict[str, List[str]] = {}\n",
        "        for (problem, i, each), gout in zip(grading_refs, g_outs):\n",
        "            gtxt = extract_final_answer(gout.outputs[0].text) if gout and gout.outputs else \"\"\n",
        "            gjson = safe_json_obj(gtxt)\n",
        "            # Reward = normalized sum of numeric grades\n",
        "            total_grade = 0.0\n",
        "            try:\n",
        "                for v in (gjson or {}).values():\n",
        "                    g = v.get(\"grade\") if isinstance(v, dict) else None\n",
        "                    if isinstance(g, (int, float)):\n",
        "                        total_grade += float(g)\n",
        "                    elif isinstance(g, str):\n",
        "                        digits = \"\".join(ch for ch in g if ch.isdigit())\n",
        "                        if digits:\n",
        "                            total_grade += float(int(digits))\n",
        "            except Exception:\n",
        "                pass\n",
        "            # Normalize by 4 * number of keypoints (exclude summary line)\n",
        "            kp = each.get(\"keypoints\") or []\n",
        "            if isinstance(kp, list):\n",
        "                num_points = max(len(kp) - 1, 1)\n",
        "            else:\n",
        "                num_points = 1\n",
        "            denom = 4.0 * float(num_points)\n",
        "            each[\"reward\"] = (total_grade / denom) if denom > 0 else 0.0\n",
        "            piece = json.dumps({\"attempt\": i, \"grading\": gjson}, ensure_ascii=False)\n",
        "            problem_to_grading_pieces.setdefault(problem, []).append(piece)\n",
        "\n",
        "        problem_to_grading_text: Dict[str, str] = {\n",
        "            p: \"\\n\".join(pieces) for p, pieces in problem_to_grading_pieces.items()\n",
        "        }\n",
        "\n",
        "        # Update per-problem totals for this batch\n",
        "        for p, rs in problem_to_rollouts.items():\n",
        "            pkey = compute_problem_key(p)\n",
        "            cur_problem_totals[pkey] = cur_problem_totals.get(pkey, 0.0) + sum(x[\"reward\"] for x in rs)\n",
        "\n",
        "        # after grading, report avg reward for batch and accumulate step total\n",
        "        batch_total = sum(x[\"reward\"] for x in rollouts)\n",
        "        step_total += batch_total\n",
        "        avg_reward = batch_total / max(1, len(rollouts))\n",
        "        print(f\"[Step {step}][Batch {batch_idx+1}/{num_batches}] Avg reward (grading-sum): {avg_reward:.4f}; Batch total: {batch_total:.4f}\")\n",
        "\n",
        "        # 7) Batched critique prompts per problem and persist per-problem experiences\n",
        "        critique_prompts: List[str] = []\n",
        "        critique_problems: List[str] = []\n",
        "        for problem, rs in problem_to_rollouts.items():\n",
        "            existing_for_problem = load_experiences_for_prompt(problem, EXPERIENCES_NEW_ROOT)\n",
        "            grading_block = problem_to_grading_text.get(problem, \"[]\")\n",
        "            existing_serialized = json.dumps(existing_for_problem or {}, ensure_ascii=False)\n",
        "            critique_prompts.append(\n",
        "                SINGLE_QUERY_CRITIQUE_TEMPLATE.format(\n",
        "                    problem=problem,\n",
        "                    grading=grading_block,\n",
        "                    answer=rs[0].get(\"groundtruth\", \"\"),\n",
        "                    experiences=existing_serialized,\n",
        "                )\n",
        "            )\n",
        "            critique_problems.append(problem)\n",
        "\n",
        "        critique_chat_prompts = [to_qwen_thinking_chat(p, grade_tokenizer) for p in critique_prompts]\n",
        "        c_params = SamplingParams(\n",
        "            temperature=TEMPERATURE,\n",
        "            max_tokens=MAX_NEW_TOKENS,\n",
        "            top_p=0.95,\n",
        "            top_k=20,\n",
        "        )\n",
        "        c_outs = grade_gen.generate(critique_chat_prompts, c_params)\n",
        "\n",
        "        for problem, cout in zip(critique_problems, c_outs):\n",
        "            ops_text = extract_final_answer(cout.outputs[0].text) if cout and cout.outputs else \"[]\"\n",
        "            ops = safe_json_array(ops_text)\n",
        "            if ops:\n",
        "                try:\n",
        "                    pkey = compute_problem_key(problem)\n",
        "                    fp = os.path.join(ops_dir, f\"{pkey}.json\")\n",
        "                    payload = {\"problem\": problem, \"ops\": []}\n",
        "                    if os.path.exists(fp):\n",
        "                        try:\n",
        "                            payload = json.load(open(fp, 'r', encoding='utf-8'))\n",
        "                            if not isinstance(payload, dict):\n",
        "                                payload = {\"problem\": problem, \"ops\": []}\n",
        "                        except Exception:\n",
        "                            payload = {\"problem\": problem, \"ops\": []}\n",
        "                    if not payload.get(\"problem\"):\n",
        "                        payload[\"problem\"] = problem\n",
        "                    cur_ops = payload.get(\"ops\") or []\n",
        "                    if not isinstance(cur_ops, list):\n",
        "                        cur_ops = []\n",
        "                    cur_ops.extend(list(ops))\n",
        "                    payload[\"ops\"] = cur_ops\n",
        "                    with open(fp, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "                except Exception as _e:\n",
        "                    print(f\"[Step {step}] WARNING: failed to write ops for problem: {problem}: {_e}\")\n",
        "\n",
        "        # End-of-step processing on the last batch\n",
        "        if (batch_idx + 1) == num_batches:\n",
        "            # 1) Confirm ops directory and count files\n",
        "            if os.path.isdir(ops_dir):\n",
        "                n_ops_files = len(glob.glob(os.path.join(ops_dir, \"*.json\")))\n",
        "                print(f\"[Step {step}] Ops directory ready: {ops_dir} ({n_ops_files} files)\")\n",
        "            else:\n",
        "                print(f\"[Step {step}] WARNING: ops dir missing: {ops_dir}\")\n",
        "\n",
        "            # 4) Rebuild NEW from baseline (updated OLD if adopted, else existing OLD), then apply ops\n",
        "            baseline_dir = EXPERIENCES_OLD_ROOT\n",
        "\n",
        "            try:\n",
        "                shutil.rmtree(EXPERIENCES_NEW_ROOT)\n",
        "            except Exception:\n",
        "                pass\n",
        "            os.makedirs(EXPERIENCES_NEW_ROOT, exist_ok=True)\n",
        "            # Clone baseline -> NEW\n",
        "            for src in glob.glob(os.path.join(baseline_dir, \"*.json\")):\n",
        "                dst = os.path.join(EXPERIENCES_NEW_ROOT, os.path.basename(src))\n",
        "                shutil.copy2(src, dst)\n",
        "            # Apply ops on top of cloned NEW by streaming per-problem JSON files\n",
        "            try:\n",
        "                for fp in glob.glob(os.path.join(ops_dir, \"*.json\")):\n",
        "                    try:\n",
        "                        rec = json.load(open(fp, 'r', encoding='utf-8'))\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    problem = rec.get(\"problem\")\n",
        "                    ops = rec.get(\"ops\") or []\n",
        "                    if not problem or not isinstance(ops, list) or not ops:\n",
        "                        continue\n",
        "                    # Load current NEW experiences for this problem (already cloned from baseline)\n",
        "                    current = load_experiences_for_prompt(problem, EXPERIENCES_NEW_ROOT)\n",
        "                    updated = dict(current)\n",
        "                    next_id = 0\n",
        "                    if updated:\n",
        "                        try:\n",
        "                            next_id = max([int(''.join([c for c in k if c.isdigit()]) or -1) for k in updated.keys()]) + 1\n",
        "                        except Exception:\n",
        "                            next_id = len(updated)\n",
        "                    for op in ops:\n",
        "                        try:\n",
        "                            opt = str(op.get(\"option\", \"\")).lower()\n",
        "                            content = str(op.get(\"experience\", \"\")).strip()\n",
        "                            if opt == \"add\" and content:\n",
        "                                updated[f\"{next_id}\"] = content\n",
        "                                next_id += 1\n",
        "                            elif opt == \"modify\":\n",
        "                                src_id = str(op.get(\"modified_from\", \"\")).strip()\n",
        "                                if src_id in updated and content:\n",
        "                                    updated[src_id] = content\n",
        "                        except Exception:\n",
        "                            continue\n",
        "                    save_experiences_for_problem(problem, updated, EXPERIENCES_NEW_ROOT)\n",
        "            except Exception as _e:\n",
        "                print(f\"[Step {step}] WARNING: failed applying ops from ops dir: {_e}\")\n",
        "            print(f\"[Step {step}] NEW experiences rebuilt from baseline with ops applied.\")\n",
        "\n",
        "                        # === Evaluate NEW vs OLD per problem; update OLD only when delta >= 0 ===\n",
        "            try:\n",
        "                # 1) Evaluate NEW using full retrieved experience sets (no globals)\n",
        "                eval_problem_totals = {}\n",
        "                GRPO_N_EVAL = 1\n",
        "                EVAL_TEMPERATURE = 0.1\n",
        "\n",
        "                for _batch_idx in range(num_batches):\n",
        "                    _start = _batch_idx * BATCH_SIZE\n",
        "                    _end = min(len(data), (_batch_idx + 1) * BATCH_SIZE)\n",
        "                    _idx_slice = step_indices[_start:_end]\n",
        "                    _batch_data = [data[i] for i in _idx_slice]\n",
        "\n",
        "                    _problems = [s['problem'] for s in _batch_data]\n",
        "                    _keys = [compute_problem_key(p) for p in _problems]\n",
        "\n",
        "                    _retrieve_k = max(TOP_K_EXPERIENCES * 3, TOP_K_EXPERIENCES)\n",
        "                    _retrieved_lists = run_search(\n",
        "                        experiences_dir=EXPERIENCES_NEW_ROOT,\n",
        "                        query=_problems,\n",
        "                        top_k=_retrieve_k,\n",
        "                        device=None,\n",
        "                        batch_size=EMBED_BATCH_SIZE,\n",
        "                        use_flash_attention_2=False,\n",
        "                        instruction=EMBED_INSTRUCTION,\n",
        "                        index_dir=EXPERIENCES_INDEX_DIR,\n",
        "                        rebuild_index=False,\n",
        "                        engine=embed_engine,\n",
        "                    )\n",
        "\n",
        "                    _formatted_batch = []\n",
        "                    for _sample, _problem_text, _current_key, _retrieved in zip(_batch_data, _problems, _keys, _retrieved_lists or []):\n",
        "                        _filtered = [it for it in (_retrieved or []) if it.get('problem_key') != _current_key]\n",
        "                        _top_items = _filtered[:TOP_K_EXPERIENCES]\n",
        "                        _prompt = PROBLEM_WITH_EXPERIENCE_TEMPLATE.format(\n",
        "                            experiences=format_experiences_for_prompt(_top_items),\n",
        "                            problem=_problem_text,\n",
        "                        )\n",
        "                        _formatted_batch.append({\"prompt\": _prompt, **_sample})\n",
        "\n",
        "                    _formatted_batch = _formatted_batch * GRPO_N_EVAL\n",
        "                    _prompts = [x[\"prompt\"] for x in _formatted_batch]\n",
        "                    _chat_prompts = [to_qwen_thinking_chat(p, gen_tokenizer) for p in _prompts]\n",
        "                    _eval_sampling_params = SamplingParams(\n",
        "                        temperature=EVAL_TEMPERATURE,\n",
        "                        max_tokens=MAX_NEW_TOKENS,\n",
        "                        top_p=0.95,\n",
        "                        top_k=20,\n",
        "                    )\n",
        "                    _eval_outs = gen.generate(_chat_prompts, _eval_sampling_params)\n",
        "                    _eval_texts = [extract_final_answer(o.outputs[0].text) for o in _eval_outs]\n",
        "\n",
        "                    _problem_to_rollouts_eval = {}\n",
        "                    for _item, _out in zip(_formatted_batch, _eval_texts):\n",
        "                        _r = dict(_item); _r[\"response\"] = _out; _r[\"reward\"] = 0.0\n",
        "                        _problem_to_rollouts_eval.setdefault(_r[\"problem\"], []).append(_r)\n",
        "\n",
        "                    _grading_prompts_eval, _grading_refs_eval = [], []\n",
        "                    for _problem, _rs in _problem_to_rollouts_eval.items():\n",
        "                        _req_block = _rs[0].get(\"keypoints\")\n",
        "                        _req_text = format_requirements_block(_req_block) if isinstance(_req_block, list) else \"\"\n",
        "                        for _i, _each in enumerate(_rs):\n",
        "                            _grading_prompts_eval.append(\n",
        "                                SINGLE_ROLLOUT_GRADING_TEMPLATE.format(\n",
        "                                    problem=_problem, response=str(_each.get(\"response\", \"\")), requirements=_req_text\n",
        "                                )\n",
        "                            )\n",
        "                            _grading_refs_eval.append((_problem, _i, _each))\n",
        "\n",
        "                    _grading_chat_prompts_eval = [to_qwen_thinking_chat(p, grade_tokenizer) for p in _grading_prompts_eval]\n",
        "                    _g_params_eval = SamplingParams(\n",
        "                        temperature=GRADING_TEMPERATURE,\n",
        "                        max_tokens=GRADING_MAX_NEW_TOKENS,\n",
        "                        top_p=0.95,\n",
        "                        top_k=20,\n",
        "                    )\n",
        "                    _g_outs_eval = grade_gen.generate(_grading_chat_prompts_eval, _g_params_eval)\n",
        "\n",
        "                    for (_problem, _i, _each), _gout in zip(_grading_refs_eval, _g_outs_eval):\n",
        "                        _gtxt = extract_final_answer(_gout.outputs[0].text) if _gout and _gout.outputs else \"\"\n",
        "                        _gjson = safe_json_obj(_gtxt)\n",
        "                        _total_grade = 0.0\n",
        "                        try:\n",
        "                            for _v in (_gjson or {}).values():\n",
        "                                _g = _v.get(\"grade\") if isinstance(_v, dict) else None\n",
        "                                if isinstance(_g, (int, float)):\n",
        "                                    _total_grade += float(_g)\n",
        "                                elif isinstance(_g, str):\n",
        "                                    _digits = \"\".join(ch for ch in _g if ch.isdigit())\n",
        "                                    if _digits:\n",
        "                                        _total_grade += float(int(_digits))\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        _kp = _each.get(\"keypoints\") or []\n",
        "                        _num_points = max(len(_kp) - 1, 1) if isinstance(_kp, list) else 1\n",
        "                        _denom = 4.0 * float(_num_points)\n",
        "                        _each[\"reward\"] = (_total_grade / _denom) if _denom > 0 else 0.0\n",
        "\n",
        "                    for _p, _rs in _problem_to_rollouts_eval.items():\n",
        "                        _pkey = compute_problem_key(_p)\n",
        "                        eval_problem_totals[_pkey] = eval_problem_totals.get(_pkey, 0.0) + sum(x[\"reward\"] for x in _rs)\n",
        "\n",
        "                # 2) Compute per-problem delta (NEW - OLD)\n",
        "                per_problem_delta = {}\n",
        "                for _pkey in set(list(eval_problem_totals.keys()) + list(cur_problem_totals.keys())):\n",
        "                    _new_total = float(eval_problem_totals.get(_pkey, 0.0))\n",
        "                    _old_total = float(cur_problem_totals.get(_pkey, 0.0))\n",
        "                    per_problem_delta[_pkey] = _new_total - _old_total\n",
        "\n",
        "                # 3) Selectively update OLD with NEW only when delta >= 0 for that problem\n",
        "                updated, skipped = 0, 0\n",
        "                for _src in sorted(glob.glob(os.path.join(EXPERIENCES_NEW_ROOT, '*.json'))):\n",
        "                    _pkey = os.path.splitext(os.path.basename(_src))[0]\n",
        "                    _delta = float(per_problem_delta.get(_pkey, 0.0))\n",
        "                    if _delta >= 0.0:\n",
        "                        _dst = os.path.join(EXPERIENCES_OLD_ROOT, os.path.basename(_src))\n",
        "                        os.makedirs(os.path.dirname(_dst), exist_ok=True)\n",
        "                        from shutil import copy2\n",
        "                        copy2(_src, _dst)\n",
        "                        updated += 1\n",
        "                    else:\n",
        "                        skipped += 1\n",
        "                print(f\"[Step {step}] OLD selectively updated from NEW: updated={updated}, skipped={skipped}\")\n",
        "\n",
        "            except Exception as __e:\n",
        "                print(f\"[Step {step}] WARNING: NEW evaluation/selective update failed: {__e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[Step {step}] WARNING: failed rebuilding NEW: {e}\")\n",
        "\n",
        "\n",
        "        print(f\"[Step {step}][Batch {batch_idx+1}/{num_batches}] Experiences updated.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vllm_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
